"""
Yahoo Finance / í•œêµ­ ì£¼ì‹ ë°ì´í„° ì¡°íšŒ ëª¨ë“ˆ

ì¬ë¬´ ì§€í‘œ, ê°€ê²©, ë‰´ìŠ¤, ë‚´ë¶€ì ê±°ë˜, ì¸ë±ìŠ¤ êµ¬ì„±ì¢…ëª© ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
ëª¨ë“  API í˜¸ì¶œì€ rate_limiterì˜ ì•ˆì „í•œ ë˜í¼ë¥¼ í†µí•´ ìˆ˜í–‰ë˜ë©°,
ê²°ê³¼ëŠ” cache ëª¨ë“ˆì„ í†µí•´ íŒŒì¼ ê¸°ë°˜ ìºì‹±ë©ë‹ˆë‹¤.

í•œêµ­ ì£¼ì‹(6ìë¦¬ ìˆ«ì ì¢…ëª©ì½”ë“œ)ì€ ìë™ìœ¼ë¡œ DART + PyKRXë¡œ ë¼ìš°íŒ…ë©ë‹ˆë‹¤.
"""
import os
import json
from datetime import datetime

import pandas as pd

from config import CACHE_DIR, CACHE_ENABLED
from cache import _get_cache_path, _read_cache, _write_cache, cache_stats
from rate_limiter import (
    safe_get_ticker_info,
    safe_get_ticker_news,
    safe_get_insider_transactions,
    safe_get_ticker_history,
    safe_get_financials,
    safe_get_balance_sheet,
    safe_batch_download,
)
from ticker_utils import is_korean_ticker, normalize_korean_ticker, is_korean_index

# ============================================================================
# ì¸ë±ìŠ¤ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ (í´ë°±ìš© í•˜ë“œì½”ë”©)
# ============================================================================

FALLBACK_SP500 = """MMM,AOS,ABT,ABBV,ACN,ADBE,AMD,AES,AFL,A,APD,ABNB,AKAM,ALB,ARE,ALGN,ALLE,LNT,ALL,GOOGL,MO,AMZN,AMCR,AEE,AEP,AXP,AIG,AMT,AWK,AMP,AME,AMGN,APH,ADI,AON,APA,APO,AAPL,AMAT,APTV,ACGL,ADM,ANET,AJG,AIZ,T,ATO,ADSK,ADP,AZO,AVB,AVY,AXON,BKR,BALL,BAC,BAX,BDX,BBY,TECH,BIIB,BLK,BX,BK,BA,BKNG,BSX,BMY,AVGO,BR,BRO,BLDR,BG,BXP,CHRW,CDNS,CZR,CPT,CPB,COF,CAH,KMX,CCL,CARR,CAT,CBOE,CBRE,CDW,COR,CNC,CNP,CF,CRL,SCHW,CHTR,CVX,CMG,CB,CHD,CI,CINF,CTAS,CSCO,C,CFG,CLX,CME,CMS,KO,CTSH,COIN,CL,CMCSA,CAG,COP,ED,STZ,CEG,COO,CPRT,GLW,CPAY,CTVA,CSGP,COST,CTRA,CRWD,CCI,CSX,CMI,CVS,DHR,DRI,DDOG,DVA,DAY,DECK,DE,DELL,DAL,DVN,DXCM,FANG,DLR,DG,DLTR,D,DPZ,DASH,DOV,DOW,DHI,DTE,DUK,DD,EMN,ETN,EBAY,ECL,EIX,EW,EA,ELV,EMR,ENPH,ETR,EOG,EPAM,EQT,EFX,EQIX,EQR,ERIE,ESS,EL,EG,EVRG,ES,EXC,EXE,EXPE,EXPD,EXR,XOM,FFIV,FDS,FICO,FAST,FRT,FDX,FIS,FITB,FSLR,FE,FI,F,FTNT,FTV,FOXA,FOX,BEN,FCX,GRMN,IT,GE,GEHC,GEV,GEN,GNRC,GD,GIS,GM,GPC,GILD,GPN,GL,GDDY,GS,HAL,HIG,HAS,HCA,DOC,HSIC,HSY,HPE,HLT,HOLX,HD,HON,HRL,HST,HWM,HPQ,HUBB,HUM,HBAN,HII,IBM,IEX,IDXX,ITW,INCY,IR,PODD,INTC,ICE,IFF,IP,IPG,INTU,ISRG,IVZ,INVH,IQV,IRM,JBHT,JBL,JKHY,J,JNJ,JCI,JPM,KVUE,KDP,KEY,KEYS,KMB,KIM,KMI,KKR,KLAC,KHC,KR,LHX,LH,LRCX,LW,LVS,LDOS,LEN,LII,LLY,LIN,LYV,LKQ,LMT,L,LOW,LULU,LYB,MTB,MPC,MKTX,MAR,MMC,MLM,MAS,MA,MTCH,MKC,MCD,MCK,MDT,MRK,META,MET,MTD,MGM,MCHP,MU,MSFT,MAA,MRNA,MHK,MOH,TAP,MDLZ,MPWR,MNST,MCO,MS,MOS,MSI,MSCI,NDAQ,NTAP,NFLX,NEM,NWSA,NWS,NEE,NKE,NI,NDSN,NSC,NTRS,NOC,NCLH,NRG,NUE,NVDA,NVR,NXPI,ORLY,OXY,ODFL,OMC,ON,OKE,ORCL,OTIS,PCAR,PKG,PANW,PARA,PH,PAYX,PAYC,PYPL,PNR,PEP,PFE,PCG,PM,PSX,PNW,PNC,POOL,PPG,PPL,PFG,PG,PGR,PLD,PRU,PEG,PTC,PSA,PHM,QRVO,PWR,QCOM,DGX,RL,RJF,RTX,O,REG,REGN,RF,RSG,RMD,RVTY,ROK,ROL,ROP,ROST,RCL,SPGI,CRM,SBAC,SLB,STX,SRE,NOW,SHW,SPG,SWKS,SJM,SW,SNA,SOLV,SO,LUV,SWK,SBUX,STT,STLD,STE,SYK,SMCI,SYF,SNPS,SYY,TMUS,TROW,TTWO,TPR,TRGP,TGT,TEL,TDY,TFX,TER,TSLA,TXN,TXT,TMO,TJX,TSCO,TT,TDG,TRV,TRMB,TFC,TYL,TSN,USB,UBER,UDR,ULTA,UNP,UAL,UPS,URI,UNH,UHS,VLO,VTR,VLTO,VRSN,VRSK,VZ,VRTX,VIAV,V,VST,VMC,WRB,GWW,WAB,WBA,WMT,DIS,WBD,WM,WAT,WEC,WFC,WELL,WST,WDC,WY,WMB,WTW,WYNN,XEL,XYL,YUM,ZBRA,ZBH,ZTS"""

FALLBACK_NASDAQ100 = """AAPL,ABNB,ADBE,ADI,ADP,ADSK,AEP,AMAT,AMGN,AMZN,ANSS,ARM,ASML,AVGO,AZN,BIIB,BKNG,BKR,CDNS,CDW,CEG,CHTR,CMCSA,COST,CPRT,CRWD,CSCO,CSGP,CSX,CTAS,CTSH,DDOG,DLTR,DXCM,EA,EXC,FANG,FAST,FTNT,GEHC,GFS,GILD,GOOG,GOOGL,HON,IDXX,ILMN,INTC,INTU,ISRG,KDP,KHC,KLAC,LIN,LRCX,LULU,MAR,MCHP,MDB,MDLZ,MELI,META,MNST,MRNA,MRVL,MSFT,MU,NFLX,NVDA,NXPI,ODFL,ON,ORLY,PANW,PAYX,PCAR,PDD,PEP,PYPL,QCOM,REGN,ROP,ROST,SBUX,SMCI,SNPS,TEAM,TMUS,TSLA,TTD,TTWO,TXN,VRSK,VRTX,WBD,WDAY,XEL,ZS"""


# ============================================================================
# ë‚´ë¶€ì ê±°ë˜ ë°ì´í„°
# ============================================================================

def _fetch_insider_trades_yf(ticker: str, limit: int = 100) -> list:
    """
    Yahoo Financeì—ì„œ ë‚´ë¶€ì ê±°ë˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

    ì£¼ìš” ê¸°ëŠ¥:
    - ìµœëŒ€ 100ê±´ì˜ ë‚´ë¶€ì ê±°ë˜ ì¡°íšŒ
    - ì£¼ë‹¹ ê±°ë˜ ê°€ê²©(transaction_price_per_share) ìë™ ê³„ì‚°
    - transaction_date, ownership_type, filing_url í•„ë“œ í¬í•¨
    - Rate limiting ì¬ì‹œë„ ë¡œì§ ì ìš©
    """
    try:
        insider_df = safe_get_insider_transactions(ticker)

        if insider_df is None or (hasattr(insider_df, 'empty') and insider_df.empty):
            return []

        trades = []
        for _, row in insider_df.head(limit).iterrows():
            shares = row.get("Shares")
            value = row.get("Value")

            # ì£¼ë‹¹ ê±°ë˜ ê°€ê²© ê³„ì‚°
            price_per_share = None
            if shares and value and shares != 0:
                try:
                    price_per_share = float(value) / float(shares)
                except (TypeError, ValueError, ZeroDivisionError):
                    pass

            trades.append({
                "insider_name": row.get("Insider") or row.get("Name"),
                "insider_title": row.get("Position") or row.get("Title"),
                "transaction_type": row.get("Transaction") or row.get("Text"),
                "shares": shares,
                "value": value,
                "transaction_price_per_share": price_per_share,
                "transaction_date": str(row.get("Start Date")) if row.get("Start Date") else None,
                "ownership_type": row.get("Ownership"),  # Direct/Indirect
                "filing_url": row.get("URL"),
            })
        return trades
    except Exception:
        return []


# ============================================================================
# ë‰´ìŠ¤ ë°ì´í„°
# ============================================================================

def _fetch_company_news_yf(ticker: str, limit: int = 50) -> list:
    """
    Yahoo Financeì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°

    ì£¼ìš” ê¸°ëŠ¥:
    - ìµœëŒ€ 50ê±´ì˜ ë‰´ìŠ¤ ì¡°íšŒ
    - summary í•„ë“œ í¬í•¨ (content.summaryì—ì„œ ì¶”ì¶œ)
    - content_type, thumbnail_url í•„ë“œ í¬í•¨
    - Rate limiting ì¬ì‹œë„ ë¡œì§ ì ìš©
    """
    try:
        news = safe_get_ticker_news(ticker)

        if not news:
            return []

        news_list = []
        for item in news[:limit]:
            # ìƒˆë¡œìš´ Yahoo Finance ë‰´ìŠ¤ êµ¬ì¡° ì§€ì›
            content = item.get("content", {})

            # ì œëª© ì¶”ì¶œ (ìƒˆ êµ¬ì¡° ìš°ì„ )
            title = content.get("title") or item.get("title", "")

            # ì†ŒìŠ¤/ë°œí–‰ì ì¶”ì¶œ
            publisher = ""
            if content.get("provider"):
                publisher = content["provider"].get("displayName", "")
            if not publisher:
                publisher = item.get("publisher", "")

            # URL ì¶”ì¶œ
            url = ""
            if content.get("canonicalUrl"):
                url = content["canonicalUrl"].get("url", "")
            if not url:
                url = item.get("link", "")

            # ë‚ ì§œ ì¶”ì¶œ
            pub_date = content.get("pubDate")
            if not pub_date and item.get("providerPublishTime"):
                try:
                    pub_date = datetime.fromtimestamp(item["providerPublishTime"]).strftime("%Y-%m-%d %H:%M:%S")
                except (TypeError, ValueError):
                    pub_date = None

            # ì¸ë„¤ì¼ URL ì¶”ì¶œ
            thumbnail_url = None
            if content.get("thumbnail"):
                thumbnail_url = content["thumbnail"].get("originalUrl")

            news_list.append({
                "title": title,
                "publisher": publisher,
                "link": url,
                "date": pub_date,
                "summary": content.get("summary", ""),
                "content_type": content.get("contentType"),
                "thumbnail_url": thumbnail_url,
            })
        return news_list
    except Exception:
        return []


# ============================================================================
# ìºì‹œëœ ë°ì´í„° ì¡°íšŒ (ë‚´ë¶€ì ê±°ë˜, ë‰´ìŠ¤)
# ============================================================================

def get_insider_trades(ticker: str, end_date: str, limit: int = 100) -> list:
    """ìºì‹œëœ ë‚´ë¶€ì ê±°ë˜ ë°ì´í„° ì¡°íšŒ"""
    if is_korean_ticker(ticker):
        from korean_data_fetcher import get_insider_trades_kr
        return get_insider_trades_kr(normalize_korean_ticker(ticker), end_date, limit)

    cache_path = _get_cache_path("insider_yf_v2", ticker, end_date, "")
    cached = _read_cache(cache_path)
    if cached is not None:
        cache_stats["hits"] += 1
        return cached

    cache_stats["misses"] += 1
    result = _fetch_insider_trades_yf(ticker, limit)
    if result:
        _write_cache(cache_path, result)
    return result


def get_company_news(ticker: str, end_date: str, limit: int = 50) -> list:
    """ìºì‹œëœ ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ"""
    if is_korean_ticker(ticker):
        from korean_data_fetcher import get_company_news_kr
        return get_company_news_kr(normalize_korean_ticker(ticker), end_date, limit)

    cache_path = _get_cache_path("news_yf_v2", ticker, end_date, "")
    cached = _read_cache(cache_path)
    if cached is not None:
        cache_stats["hits"] += 1
        return cached

    cache_stats["misses"] += 1
    result = _fetch_company_news_yf(ticker, limit)
    if result:
        _write_cache(cache_path, result)
    return result


# ============================================================================
# ì¬ë¬´ ì§€í‘œ (íŒŒìƒ ì§€í‘œ í¬í•¨)
# ============================================================================

def _calculate_derived_metrics(ticker: str, info: dict) -> dict:
    """
    ì¬ë¬´ì œí‘œ ê¸°ë°˜ íŒŒìƒ ì§€í‘œ ê³„ì‚°

    ROIC, Interest Coverage, Cash Ratio ë“± Yahoo Finance infoì—ì„œ
    ì§ì ‘ ì œê³µí•˜ì§€ ì•ŠëŠ” ì§€í‘œë“¤ì„ ì¬ë¬´ì œí‘œì—ì„œ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    derived = {
        "return_on_invested_capital": None,
        "interest_coverage": None,
        "cash_ratio": None,
        "operating_cash_flow_ratio": None,
        "asset_turnover": None,
        "research_and_development": None,
        "research_and_development_ratio": None,
    }

    try:
        income_stmt = safe_get_financials(ticker)
        balance_sheet = safe_get_balance_sheet(ticker)

        if income_stmt is None or (hasattr(income_stmt, 'empty') and income_stmt.empty):
            return derived
        if balance_sheet is None or (hasattr(balance_sheet, 'empty') and balance_sheet.empty):
            return derived

        # ìµœì‹  ê¸°ê°„ì˜ ë°ì´í„° ì‚¬ìš©
        latest_col = income_stmt.columns[0] if len(income_stmt.columns) > 0 else None
        if latest_col is None:
            return derived

        # ROIC = NOPAT / Invested Capital
        try:
            ebit = None
            for ebit_name in ["EBIT", "Operating Income"]:
                if ebit_name in income_stmt.index:
                    val = income_stmt.loc[ebit_name, latest_col]
                    if pd.notna(val):
                        ebit = float(val)
                        break

            if ebit:
                tax_rate = 0.21  # ë¯¸êµ­ ë²•ì¸ì„¸ìœ¨ ê°€ì • (í•œêµ­ ì¢…ëª©ì€ korean_data_fetcherë¡œ ë¼ìš°íŒ…ë˜ì–´ ì—¬ê¸° ë„ë‹¬í•˜ì§€ ì•ŠìŒ)
                nopat = ebit * (1 - tax_rate)

                total_equity = None
                for eq_name in ["Stockholders Equity", "Total Stockholder Equity", "Total Equity Gross Minority Interest"]:
                    if eq_name in balance_sheet.index:
                        val = balance_sheet.loc[eq_name, latest_col] if latest_col in balance_sheet.columns else None
                        if val is not None and pd.notna(val):
                            total_equity = float(val)
                            break

                total_debt = info.get("totalDebt", 0) or 0
                cash = info.get("totalCash", 0) or 0

                if total_equity:
                    invested_capital = total_equity + total_debt - cash
                    if invested_capital > 0:
                        derived["return_on_invested_capital"] = nopat / invested_capital
        except Exception:
            pass

        # Interest Coverage = EBIT / Interest Expense
        try:
            ebit = None
            for ebit_name in ["EBIT", "Operating Income"]:
                if ebit_name in income_stmt.index:
                    val = income_stmt.loc[ebit_name, latest_col]
                    if pd.notna(val):
                        ebit = float(val)
                        break

            interest_expense = None
            for int_name in ["Interest Expense", "Interest Expense Non Operating"]:
                if int_name in income_stmt.index:
                    val = income_stmt.loc[int_name, latest_col]
                    if pd.notna(val):
                        interest_expense = abs(float(val))
                        break

            if ebit and interest_expense and interest_expense > 0:
                derived["interest_coverage"] = ebit / interest_expense
        except Exception:
            pass

        # Cash Ratio = Cash / Current Liabilities
        try:
            cash = info.get("totalCash", 0) or 0
            current_liabilities = None
            for cl_name in ["Current Liabilities", "Total Current Liabilities"]:
                if cl_name in balance_sheet.index:
                    val = balance_sheet.loc[cl_name, latest_col] if latest_col in balance_sheet.columns else None
                    if val is not None and pd.notna(val):
                        current_liabilities = float(val)
                        break

            if cash and current_liabilities and current_liabilities > 0:
                derived["cash_ratio"] = cash / current_liabilities
        except Exception:
            pass

        # Operating Cash Flow Ratio = Operating Cash Flow / Current Liabilities
        try:
            op_cf = info.get("operatingCashflow", 0) or 0
            if op_cf and current_liabilities and current_liabilities > 0:
                derived["operating_cash_flow_ratio"] = op_cf / current_liabilities
        except Exception:
            pass

        # Asset Turnover = Revenue / Total Assets
        try:
            revenue = info.get("totalRevenue", 0) or 0
            total_assets = None
            if "Total Assets" in balance_sheet.index:
                val = balance_sheet.loc["Total Assets", latest_col] if latest_col in balance_sheet.columns else None
                if val is not None and pd.notna(val):
                    total_assets = float(val)

            if revenue and total_assets and total_assets > 0:
                derived["asset_turnover"] = revenue / total_assets
        except Exception:
            pass

        # R&D (Research and Development) - Fisher ë¶„ì„ìš©
        try:
            rd_value = None
            for rd_name in ["Research And Development", "Research Development", "Research And Development Expenses"]:
                if rd_name in income_stmt.index:
                    val = income_stmt.loc[rd_name, latest_col]
                    if pd.notna(val):
                        rd_value = float(val)
                        break

            if rd_value and rd_value > 0:
                derived["research_and_development"] = rd_value
                total_rev = info.get("totalRevenue")
                if total_rev and total_rev > 0:
                    derived["research_and_development_ratio"] = rd_value / total_rev
        except Exception:
            pass

    except Exception:
        pass

    return derived


def _fetch_financial_metrics_yf(ticker: str) -> dict:
    """
    Yahoo Financeì—ì„œ ì¬ë¬´ ì§€í‘œ ê°€ì ¸ì˜¤ê¸°

    ì£¼ìš” ê¸°ëŠ¥:
    - ê¸°ë³¸ + í™•ì¥ ì¬ë¬´ ì§€í‘œ (enterprise_value, eps, book_value_per_share ë“±)
    - ROIC, Interest Coverage, Cash Ratio ë“± íŒŒìƒ ì§€í‘œ ê³„ì‚°
    - ì†Œìœ ê¶Œ/ê³µë§¤ë„ ì§€í‘œ í¬í•¨
    - ê¸°ìˆ ì  ì§€í‘œ í¬í•¨ (52ì£¼ ê³ /ì €, ì´ë™í‰ê·  ë“±)
    """
    try:
        info = safe_get_ticker_info(ticker)

        if not info:
            return None

        # íŒŒìƒ ì§€í‘œ ê³„ì‚° (ROIC, Interest Coverage ë“±)
        derived = _calculate_derived_metrics(ticker, info)

        # ì‹œê°€ì´ì•¡ê³¼ FCF ë¯¸ë¦¬ ì¶”ì¶œ
        market_cap = info.get("marketCap")
        free_cash_flow = info.get("freeCashflow")
        shares_outstanding = info.get("sharesOutstanding")
        total_debt = info.get("totalDebt")

        return {
            "ticker": ticker,

            # ===== ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ =====
            "price_to_earnings_ratio": info.get("trailingPE") or info.get("forwardPE"),
            "price_to_book_ratio": info.get("priceToBook"),
            "price_to_sales_ratio": info.get("priceToSalesTrailing12Months"),
            "peg_ratio": info.get("pegRatio"),
            "enterprise_value_to_ebitda": info.get("enterpriseToEbitda"),
            "enterprise_value": info.get("enterpriseValue"),
            "enterprise_value_to_revenue": info.get("enterpriseToRevenue"),

            # ===== ìˆ˜ìµì„± ì§€í‘œ =====
            "return_on_equity": info.get("returnOnEquity"),
            "return_on_assets": info.get("returnOnAssets"),
            "return_on_invested_capital": derived.get("return_on_invested_capital"),
            "gross_margin": info.get("grossMargins"),
            "operating_margin": info.get("operatingMargins"),
            "net_margin": info.get("profitMargins"),
            "ebitda": info.get("ebitda"),
            "ebitda_margins": info.get("ebitdaMargins"),

            # ===== ì„±ì¥ ì§€í‘œ =====
            "revenue_growth": info.get("revenueGrowth"),
            "earnings_growth": info.get("earningsGrowth"),
            "earnings_per_share_growth": info.get("earningsQuarterlyGrowth"),

            # ===== ì¬ë¬´ ê±´ì „ì„± =====
            "current_ratio": info.get("currentRatio"),
            "quick_ratio": info.get("quickRatio"),
            "debt_to_equity": info.get("debtToEquity") / 100 if info.get("debtToEquity") else None,
            "interest_coverage": derived.get("interest_coverage"),
            "cash_ratio": derived.get("cash_ratio"),
            "operating_cash_flow_ratio": derived.get("operating_cash_flow_ratio"),
            "asset_turnover": derived.get("asset_turnover"),
            "debt_to_assets": (total_debt / (total_debt + market_cap)) if total_debt and market_cap else None,

            # ===== ë°°ë‹¹ =====
            "dividend_yield": info.get("dividendYield"),
            "payout_ratio": info.get("payoutRatio"),

            # ===== ì‹œê°€ì´ì•¡ ë° ì£¼ì‹ ì •ë³´ =====
            "market_cap": market_cap,
            "shares_outstanding": shares_outstanding,
            "float_shares": info.get("floatShares"),

            # ===== í˜„ê¸ˆíë¦„ =====
            "free_cash_flow": free_cash_flow,
            "free_cash_flow_yield": (free_cash_flow / market_cap) if free_cash_flow and market_cap else None,
            "free_cash_flow_per_share": (free_cash_flow / shares_outstanding) if free_cash_flow and shares_outstanding else None,
            "operating_cashflow": info.get("operatingCashflow"),

            # ===== ë¶€ì±„/í˜„ê¸ˆ =====
            "total_debt": total_debt,
            "total_cash": info.get("totalCash"),
            "total_cash_per_share": info.get("totalCashPerShare"),

            # ===== ë§¤ì¶œ/ìˆ˜ìµ =====
            "total_revenue": info.get("totalRevenue"),
            "revenue_per_share": info.get("revenuePerShare"),
            "earnings_per_share": info.get("trailingEps"),
            "forward_eps": info.get("forwardEps"),
            "book_value_per_share": info.get("bookValue"),

            # ===== ì†Œìœ ê¶Œ/ê³µë§¤ë„ ì§€í‘œ =====
            "held_percent_insiders": info.get("heldPercentInsiders"),
            "held_percent_institutions": info.get("heldPercentInstitutions"),
            "short_ratio": info.get("shortRatio"),
            "short_percent_of_float": info.get("shortPercentOfFloat"),

            # ===== ê¸°ìˆ ì  ì§€í‘œ =====
            "beta": info.get("beta"),
            "52_week_high": info.get("fiftyTwoWeekHigh"),
            "52_week_low": info.get("fiftyTwoWeekLow"),
            "52_week_change": info.get("52WeekChange"),
            "50_day_average": info.get("fiftyDayAverage"),
            "200_day_average": info.get("twoHundredDayAverage"),

            # ===== ì„¹í„°/ì¸ë”ìŠ¤íŠ¸ë¦¬ ì •ë³´ =====
            "sector": info.get("sector"),
            "industry": info.get("industry"),

            # ===== R&D ì§€í‘œ (Fisher ë¶„ì„ìš©) =====
            "research_and_development": derived.get("research_and_development"),
            "research_and_development_ratio": derived.get("research_and_development_ratio"),
        }
    except Exception:
        return None


# ============================================================================
# ê°€ê²© ë°ì´í„°
# ============================================================================

def _fetch_prices_yf(ticker: str, start_date: str, end_date: str) -> list:
    """Yahoo Financeì—ì„œ ê°€ê²© ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë‹¨ì¼ í‹°ì»¤)"""
    try:
        df = safe_get_ticker_history(ticker, start_date, end_date)

        if df is None or df.empty:
            return []

        prices = []
        for date, row in df.iterrows():
            prices.append({
                "time": date.strftime("%Y-%m-%d"),
                "open": float(row["Open"]),
                "high": float(row["High"]),
                "low": float(row["Low"]),
                "close": float(row["Close"]),
                "volume": int(row["Volume"]),
            })
        return prices
    except Exception:
        return []


def batch_fetch_prices(tickers: list, start_date: str, end_date: str) -> dict:
    """
    ì—¬ëŸ¬ ì¢…ëª©ì˜ ê°€ê²© ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸° (ë°°ì¹˜ ì²˜ë¦¬)

    í•œêµ­ í‹°ì»¤ì™€ í•´ì™¸ í‹°ì»¤ë¥¼ ìë™ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê°ê°ì˜ ë°ì´í„°ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
    - í•´ì™¸: yf.download() (ë©€í‹° í‹°ì»¤ ë°°ì¹˜)
    - í•œêµ­: PyKRX (ìˆœì°¨ í˜¸ì¶œ)

    Returns:
        dict: {ticker: [price_list]} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬
    """
    if not tickers:
        return {}

    # í•œêµ­/í•´ì™¸ í‹°ì»¤ ë¶„ë¦¬
    kr_tickers = [t for t in tickers if is_korean_ticker(t)]
    us_tickers = [t for t in tickers if not is_korean_ticker(t)]

    result = {}

    # í•œêµ­ í‹°ì»¤ ì²˜ë¦¬
    if kr_tickers:
        from korean_data_fetcher import batch_fetch_prices_kr
        kr_normalized = [normalize_korean_ticker(t) for t in kr_tickers]
        kr_result = batch_fetch_prices_kr(kr_normalized, start_date, end_date)
        # ì›ë˜ í‹°ì»¤ëª…ìœ¼ë¡œ ë§¤í•‘ (ì •ê·œí™” ì „ ì´ë¦„ ìœ ì§€)
        for orig, norm in zip(kr_tickers, kr_normalized):
            if norm in kr_result:
                result[orig] = kr_result[norm]

    # í•´ì™¸ í‹°ì»¤ê°€ ì—†ìœ¼ë©´ í•œêµ­ ê²°ê³¼ë§Œ ë°˜í™˜
    if not us_tickers:
        return result

    try:
        print(f"ğŸ“Š ê°€ê²© ë°ì´í„° ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì¤‘... ({len(us_tickers)}ê°œ í•´ì™¸ ì¢…ëª©)")

        df = safe_batch_download(
            tickers=us_tickers,
            start=start_date,
            end=end_date,
            group_by='ticker',
        )

        if df is None or df.empty:
            print("   âš ï¸  ê°€ê²© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return result

        # ë‹¨ì¼ í‹°ì»¤ì¸ ê²½ìš° ì»¬ëŸ¼ êµ¬ì¡°ê°€ ë‹¤ë¦„
        if len(us_tickers) == 1:
            ticker = us_tickers[0]
            prices = []
            for date, row in df.iterrows():
                try:
                    prices.append({
                        "time": date.strftime("%Y-%m-%d"),
                        "open": float(row["Open"]),
                        "high": float(row["High"]),
                        "low": float(row["Low"]),
                        "close": float(row["Close"]),
                        "volume": int(row["Volume"]),
                    })
                except (KeyError, ValueError, TypeError):
                    continue
            if prices:
                result[ticker] = prices
        else:
            # ë©€í‹° í‹°ì»¤: group_by='ticker'ë¡œ ì¸í•´ (ticker, column) í˜•íƒœì˜ ë©€í‹°ì¸ë±ìŠ¤
            for ticker in us_tickers:
                try:
                    if ticker not in df.columns.get_level_values(0):
                        continue

                    ticker_df = df[ticker]
                    if ticker_df.empty or ticker_df['Close'].isna().all():
                        continue

                    prices = []
                    for date, row in ticker_df.iterrows():
                        try:
                            if pd.isna(row["Close"]):
                                continue
                            prices.append({
                                "time": date.strftime("%Y-%m-%d"),
                                "open": float(row["Open"]) if not pd.isna(row["Open"]) else 0,
                                "high": float(row["High"]) if not pd.isna(row["High"]) else 0,
                                "low": float(row["Low"]) if not pd.isna(row["Low"]) else 0,
                                "close": float(row["Close"]),
                                "volume": int(row["Volume"]) if not pd.isna(row["Volume"]) else 0,
                            })
                        except (KeyError, ValueError, TypeError):
                            continue

                    if prices:
                        result[ticker] = prices
                except Exception:
                    continue

        print(f"   âœ… {len(result)}ê°œ ì¢…ëª© ê°€ê²© ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        return result

    except Exception as e:
        print(f"   âš ï¸  ë°°ì¹˜ ê°€ê²© ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {}


# ============================================================================
# ìºì‹œëœ ë°ì´í„° ì¡°íšŒ (ì¬ë¬´ ì§€í‘œ, ê°€ê²©)
# ============================================================================

def get_financial_metrics(ticker, end_date, period="ttm", limit=10):
    """ìºì‹œëœ financial metrics ì¡°íšŒ (Yahoo Finance / í•œêµ­ DART+PyKRX)"""
    if is_korean_ticker(ticker):
        from korean_data_fetcher import get_financial_metrics_kr
        kr_ticker = normalize_korean_ticker(ticker)
        cache_path = _get_cache_path("metrics_kr", kr_ticker, end_date, "")
        cached = _read_cache(cache_path)
        if cached is not None:
            cache_stats["hits"] += 1
            return [cached]
        cache_stats["misses"] += 1
        result = get_financial_metrics_kr(kr_ticker, end_date)
        if result:
            _write_cache(cache_path, result)
            return [result]
        return []

    cache_path = _get_cache_path("metrics_yf", ticker, end_date, "")
    cached = _read_cache(cache_path)
    if cached is not None:
        cache_stats["hits"] += 1
        return [cached]

    cache_stats["misses"] += 1
    result = _fetch_financial_metrics_yf(ticker)
    if result:
        _write_cache(cache_path, result)
        return [result]
    return []


def get_prices(ticker, start_date, end_date):
    """ìºì‹œëœ ê°€ê²© ë°ì´í„° ì¡°íšŒ (Yahoo Finance / í•œêµ­ PyKRX)"""
    if is_korean_ticker(ticker):
        from korean_data_fetcher import get_prices_kr
        kr_ticker = normalize_korean_ticker(ticker)
        cache_path = _get_cache_path("prices_kr", kr_ticker, end_date, start_date)
        cached = _read_cache(cache_path)
        if cached is not None:
            cache_stats["hits"] += 1
            return cached
        cache_stats["misses"] += 1
        result = get_prices_kr(kr_ticker, start_date, end_date)
        if result:
            _write_cache(cache_path, result)
        return result

    cache_path = _get_cache_path("prices_yf", ticker, end_date, start_date)
    cached = _read_cache(cache_path)
    if cached is not None:
        cache_stats["hits"] += 1
        return cached

    cache_stats["misses"] += 1
    result = _fetch_prices_yf(ticker, start_date, end_date)
    if result:
        _write_cache(cache_path, result)
    return result


# ============================================================================
# Wikipediaì—ì„œ ì¸ë±ìŠ¤ êµ¬ì„±ì¢…ëª© ë™ì  ì¡°íšŒ
# ============================================================================

def fetch_sp500_tickers_from_wikipedia():
    """Wikipediaì—ì„œ S&P 500 êµ¬ì„±ì¢…ëª© ê°€ì ¸ì˜¤ê¸°"""
    try:
        import requests
        import re

        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        headers = {"User-Agent": "Mozilla/5.0 (compatible; StockAnalyzer/1.0)"}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        html = response.text

        pattern = r'<td[^>]*>\s*<a[^>]*href="/wiki/[^"]*"[^>]*title="[^"]*"[^>]*>([A-Z]{1,5})</a>'
        matches = re.findall(pattern, html)

        if len(matches) >= 400:
            seen = set()
            tickers = []
            for t in matches:
                if t not in seen and len(t) <= 5:
                    seen.add(t)
                    tickers.append(t.replace('.', '-'))
            return tickers[:505]

        return None
    except Exception as e:
        print(f"âš ï¸  Wikipediaì—ì„œ S&P 500 ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


def fetch_nasdaq100_tickers_from_wikipedia():
    """Wikipediaì—ì„œ NASDAQ-100 êµ¬ì„±ì¢…ëª© ê°€ì ¸ì˜¤ê¸°"""
    try:
        import requests
        import re

        url = "https://en.wikipedia.org/wiki/Nasdaq-100"
        headers = {"User-Agent": "Mozilla/5.0 (compatible; StockAnalyzer/1.0)"}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        html = response.text

        pattern = r'<td[^>]*>\s*<a[^>]*href="/wiki/[^"]*"[^>]*>([A-Z]{1,5})</a>'
        matches = re.findall(pattern, html)

        if len(matches) >= 80:
            seen = set()
            tickers = []
            for t in matches:
                if t not in seen and len(t) <= 5 and t.isupper():
                    seen.add(t)
                    tickers.append(t)
            if len(tickers) >= 90:
                return tickers[:105]

        return None
    except Exception as e:
        print(f"âš ï¸  Wikipediaì—ì„œ NASDAQ-100 ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


def get_index_tickers(index_name: str, use_cache: bool = True) -> list:
    """
    ì¸ë±ìŠ¤ êµ¬ì„±ì¢…ëª© í‹°ì»¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°

    Args:
        index_name: 'sp500', 'nasdaq100', 'kospi', 'kosdaq', 'kospi200', 'kosdaq150'
        use_cache: í‹°ì»¤ ëª©ë¡ ìºì‹œ ì‚¬ìš© ì—¬ë¶€

    Returns:
        í‹°ì»¤ ëª©ë¡ (ë¦¬ìŠ¤íŠ¸). í•œêµ­ ì¸ë±ìŠ¤ëŠ” PyKRX, ë¯¸êµ­ ì¸ë±ìŠ¤ëŠ” Wikipediaì—ì„œ ì¡°íšŒ.
    """
    today = datetime.now().strftime("%Y-%m-%d")
    cache_path = os.path.join(CACHE_DIR, f"tickers_{index_name}_{today}.json")

    if use_cache and CACHE_ENABLED and os.path.exists(cache_path):
        try:
            with open(cache_path, 'r') as f:
                cached = json.load(f)
                print(f"ğŸ“‹ {index_name.upper()} í‹°ì»¤ ëª©ë¡: ìºì‹œì—ì„œ ë¡œë“œ ({len(cached)}ê°œ)")
                return cached
        except Exception:
            pass

    # í•œêµ­ ì¸ë±ìŠ¤ ì§€ì› (kospi, kosdaq, krx)
    if is_korean_index(index_name):
        from korean_data_fetcher import get_index_tickers_kr

        # krx: KOSPI + KOSDAQ ì „ì²´ í•©ì‚°
        if index_name.lower() == "krx":
            print(f"ğŸ“‹ KRX (KOSPI + KOSDAQ) ì „ì²´ êµ¬ì„±ì¢…ëª©ì„ PyKRXì—ì„œ ì¡°íšŒ ì¤‘...")
            kospi_tickers = get_index_tickers_kr("kospi") or []
            kosdaq_tickers = get_index_tickers_kr("kosdaq") or []
            # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
            seen = set()
            kr_tickers = []
            for t in kospi_tickers + kosdaq_tickers:
                if t not in seen:
                    seen.add(t)
                    kr_tickers.append(t)
            if kr_tickers:
                print(f"   âœ… KOSPI {len(kospi_tickers)}ê°œ + KOSDAQ {len(kosdaq_tickers)}ê°œ = ì´ {len(kr_tickers)}ê°œ ì¢…ëª© ì¡°íšŒ ì™„ë£Œ")
        else:
            print(f"ğŸ“‹ {index_name.upper()} êµ¬ì„±ì¢…ëª©ì„ PyKRXì—ì„œ ì¡°íšŒ ì¤‘...")
            kr_tickers = get_index_tickers_kr(index_name)

        if kr_tickers:
            if index_name.lower() != "krx":
                print(f"   âœ… PyKRXì—ì„œ {len(kr_tickers)}ê°œ ì¢…ëª© ì¡°íšŒ ì™„ë£Œ")
            if use_cache and CACHE_ENABLED:
                try:
                    if not os.path.exists(CACHE_DIR):
                        os.makedirs(CACHE_DIR)
                    with open(cache_path, 'w') as f:
                        json.dump(kr_tickers, f)
                except Exception:
                    pass
        else:
            print(f"   âš ï¸ {index_name.upper()} ì¢…ëª© ì¡°íšŒ ì‹¤íŒ¨")
            kr_tickers = []
        return kr_tickers

    tickers = None
    if index_name == "sp500":
        print("ğŸ“‹ S&P 500 êµ¬ì„±ì¢…ëª©ì„ Wikipediaì—ì„œ ì¡°íšŒ ì¤‘...")
        tickers = fetch_sp500_tickers_from_wikipedia()
        fallback = FALLBACK_SP500
    elif index_name == "nasdaq100":
        print("ğŸ“‹ NASDAQ-100 êµ¬ì„±ì¢…ëª©ì„ Wikipediaì—ì„œ ì¡°íšŒ ì¤‘...")
        tickers = fetch_nasdaq100_tickers_from_wikipedia()
        fallback = FALLBACK_NASDAQ100
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ì¸ë±ìŠ¤: {index_name}")

    if tickers is None:
        tickers = [t.strip() for t in fallback.split(',')]
        print(f"   í´ë°± ëª©ë¡ ì‚¬ìš©: {len(tickers)}ê°œ ì¢…ëª©")
    else:
        print(f"   âœ… Wikipediaì—ì„œ {len(tickers)}ê°œ ì¢…ëª© ì¡°íšŒ ì™„ë£Œ")
        if use_cache and CACHE_ENABLED:
            try:
                if not os.path.exists(CACHE_DIR):
                    os.makedirs(CACHE_DIR)
                with open(cache_path, 'w') as f:
                    json.dump(tickers, f)
            except Exception:
                pass

    return tickers


def sort_tickers_by_market_cap(tickers, top_n=0):
    """í‹°ì»¤ë¥¼ ì‹œê°€ì´ì•¡ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (í•œêµ­/í•´ì™¸ ìë™ ë¶„ê¸°)"""
    print(f"ğŸ“Š {len(tickers)}ê°œ ì¢…ëª©ì„ ì‹œê°€ì´ì•¡ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ì¤‘...")

    market_caps = {}
    batch_size = 50

    for i in range(0, len(tickers), batch_size):
        batch = tickers[i:i+batch_size]
        for ticker in batch:
            try:
                if is_korean_ticker(ticker):
                    from korean_data_fetcher import get_market_cap_kr
                    kr_ticker = normalize_korean_ticker(ticker)
                    cap = get_market_cap_kr(kr_ticker, datetime.now().strftime("%Y-%m-%d"))
                    market_caps[ticker] = cap or 0
                else:
                    info = safe_get_ticker_info(ticker)
                    market_cap = info.get("marketCap", 0) or 0 if info else 0
                    market_caps[ticker] = market_cap
            except Exception:
                market_caps[ticker] = 0

    sorted_tickers = sorted(tickers, key=lambda t: market_caps.get(t, 0), reverse=True)

    if top_n > 0:
        sorted_tickers = sorted_tickers[:top_n]

    print(f"   ì‹œê°€ì´ì•¡ ìƒìœ„ 10ê°œ: {sorted_tickers[:10]}")
    return sorted_tickers
